# Local AI Writer App

A web application that generates creative content (blog intros, tweets, stories) using local LLM inference.

## Features

- ğŸ“ Generate blog intros, tweets, and short stories
- ğŸ¯ Topic-based content generation
- ğŸšï¸ Adjustable temperature for creativity control
- ğŸ’» 100% local inference - no cloud APIs needed
- âš¡ Real-time generation with loading states
- ğŸ“Š Output logging to track generations

## Tech Stack

- Next.js 14 with App Router
- TypeScript
- Tailwind CSS
- Ollama for local LLM inference
- Mistral 7B as the base model

## Prerequisites

1. Node.js 18+ installed
2. [Ollama](https://ollama.ai/) installed
3. Mistral 7B model pulled via Ollama

## Setup Instructions

1. Clone the repository:
   ```bash
   git clone <your-repo-url>
   cd local-ai-app
   ```

2. Install dependencies:
   ```bash
   npm install
   ```

3. Pull the Mistral model (if not already done):
   ```bash
   ollama pull mistral
   ```

4. Start the development server:
   ```bash
   npm run dev
   ```

5. Open [http://localhost:3000](http://localhost:3000) in your browser

## Project Structure

```
â”œâ”€â”€ app/                  # Next.js app directory
â”‚   â”œâ”€â”€ api/             # API routes
â”‚   â”œâ”€â”€ components/      # React components
â”‚   â”œâ”€â”€ hooks/           # Custom React hooks
â”‚   â””â”€â”€ page.tsx         # Main page
â”œâ”€â”€ public/              # Static assets
â”œâ”€â”€ styles/             # Global styles
â””â”€â”€ types/              # TypeScript type definitions
```

## License

MIT
